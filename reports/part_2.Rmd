---
title: "Part 2"
author: "Ezequiel Hermosillo"
output:
  html_document:
    code_folding: hide
    df_print: paged
    theme: sandstone
  pdf_document: default
---

```{r setup, include=FALSE}
options(knitr.duplicate.label = "allow")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(tidyverse)
library(lubridate)
library(DBI)
library(purrr)
library(stargazer)
library(DT)


```

Property taxes through out the United States are inequitable where low-value properties have higher tax rates compared to high valued properties.This inaccurate calculation is known as regressivity that has resulted in owners of low property value pay too much (Haris School of Public Policy). Detroit, Michigan has undergone a foreclosure crisis that has effected and displaced Black residents who make up 80% of the population in the city (Atuahene and Berry 2019). 

The data analyzed is focused on four year from 2015-2018. The analysis conducted is only focused on single family homes classified as 401 officially property a property of land that has a residential  structure. In other words it is a residential property. Table 1 is the descriptive statistics for every year showing that number of sales increased for every year. Additionally, as the sale prices median rose the median asses value declined between the period analyzed. The minimum sale price for a house is 4,000 and the max is 395,000.

The first figure illustrates the decline of assessed values on the same graph marking the average sales and median ratio broken by waters of the years analyzed. The ratio is converted to a percent. There are large drops of the average ratio in the year 2016 and at the end of 2017. The largest drop occured in 2016. 

```{r}
#example connection to database. note that you have to download the database from onedrive

#test 


con <- DBI::dbConnect(RSQLite::SQLite(), "database/detroit.sqlite")

# sales tbl

dplyr::tbl(con, 'sales')

# convert to tibble
#dplyr::tbl(con, 'sales') %>% dplyr::collect()

# sql query

dplyr::tbl(con, 'sales') %>% count(year(sale_date))

#dplyr::tbl(con, 'sales') %>% count(year(sale_date)) %>% show_query()


```
```{r Part I: Exploratory}

#shows list of tables
as.data.frame(dbListTables(con))


#Looking at table data and variables 
dbReadTable(con, 'sales')
dbReadTable(con, 'assessments')
#dbReadTable(con, 'blight')
dbReadTable(con, 'foreclosures')
#dbReadTable(con, 'parcels')
#dbReadTable(con, 'parcels_historic')


##Merging tables/Data cleaning

#making tables
sales <- dplyr::tbl(con, 'sales') %>% dplyr::collect()
assessments <- dplyr::tbl(con, 'assessments') %>% dplyr::collect()
foreclosures <- dplyr::tbl(con, 'foreclosures') %>% dplyr::collect()
#rename column
assessments <- rename(assessments, parcel_num = PARCELNO,
                      assessed_value = ASSESSEDVALUE,
                      tax_value = TAXABLEVALUE)

# sale dat to only include the year
sales <- sales %>%
  mutate(
    year_sale = year(`sale_date`)
  )
                    
sales <- sales %>% filter(year_sale >= 2015 & year_sale<= 2018)

sales %>% distinct(`sale_terms`)

sales <- sales %>% filter(str_detect(`sale_terms`, 'VALID ARMS LENGTH'))


db1 <- inner_join(sales, assessments, by = "parcel_num")

db1 <- db1 %>% rename(tax_year = year)




```



```{r}
#sales ratio
#The sales ratio is defined as the assessed value 
#of a property divided by the sale price.

db1 <- db1 %>% mutate(ratio = assessed_value/sale_price)

min(db2$sale_price)
max(db2$sale_price)

#db1 <- db1 %>% mutate(sale_year = (sale_date))


#count property type
db1 %>% count(`property_c`, sort=TRUE)
```

```{r Data Cleaning Cont}
#filtered out based on https://www.michigan.gov/documents/treasury/CAMA_Data_Standards_Final_121719_674322_7.pdf

db1 <- db1 %>% filter(!property_c %in% c(402,483,447,446,404,465,448,461,403))

db1 <- db1 %>% filter(year_sale >= 2015 & year_sale <= 2018)

db1 <- distinct(db1)


#creation of second data base with foreclosure data

foreclosures <- rename(foreclosures, parcel_num = prop_parcelnum)

foreclosures <- foreclosures %>% select(parcel_num,prop_addr,`2015`,`2016`,`2017`,`2018`)


#not sure how to wrap my head around of the dummy variables for foreclosure 

#removing NAs and making a dummy variables
foreclosures <-foreclosures %>%
  mutate(foreclose = coalesce(`2015`,`2016`,`2017`,`2018`))

db2 <- inner_join(db1, foreclosures, by = "parcel_num")

db2<- mutate_at(db2, c("foreclose"), ~replace(., is.na(.), 0))


db2 <- db2 %>% filter(property_c == 401 & assessed_value >= 2000 & sale_price >= 4000)

db3 <- db2 %>%
  arrange(parcel_num, forclose == 1) %>%
  filter(duplicated(pracel_num) == FALSE)
              

#Stats/tables/attempt in making graphs
#outside of making a scatter plot not sure how useful other graphs will be. They don't seem to be particularly readable.


#tbl1 <- db1 %>% group_by(year(sale_date)) %>%
 # select(sale_price, assessed_value) %>%
  #  summarise(sale_price_median = median(sale_price),
   #           assesed_median = median(assessed_value),
    #          sale_price_avg = mean(sale_price),
     #         assesed_avg = mean(assessed_value))


tbl1 <- db2 %>%
  group_by(year_sale) %>%
  select(sale_price, assessed_value, year_sale, foreclose) %>%
  filter(foreclose == 1) %>%
    summarise(sale_price_median = median(sale_price),
              assesed_median = median(assessed_value))
tbl2 <- db2 %>% count(year_sale)

tbl1 <- inner_join(tbl1, tbl2, by ="year_sale")

tbl1 <- tbl1 %>% rename(num_sale = n)
  
datatable(tbl1, options = list(dom = 't'),  colnames = c('Year of Sale', 'Sale Price Mdiean', 'Assessed Median', 'Number of Arm Length Sales'))


# %>% group_by(`propclass`) %>%
 # summarize(n = n()) %>%
  #slice_max(order_by = `n`, n= 5)



#line graph


tbl3 <- db2 %>% group_by(sale_date) %>% 
 filter(foreclose == 1) %>%
  mutate(qtr = quarter(sale_date, with_year = TRUE)) %>%
  group_by(qtr) %>%
  summarise(avg_ratio = mean(ratio * 100),
            med_ratio = median(ratio * 100)) 

 
ggplot(tbl3.., aes(x = qtr)) +
  geom_line(aes(y = avg_ratio), color = "steelblue") +
  geom_line(aes(y = med_ratio), color = "darkred") +
  geom_point(aes(y = avg_ratio), color = "steelblue") +
  geom_point(aes(y = med_ratio), color = "darkred")
  
                              


ggplot(tbl2, aes(x=year_sale, y =n)) +
  geom_point() +
  geom_line()

```


```{r Graphs}


#boxplot
ggplot(data = db1) + 
  geom_boxplot(mapping = aes(x = factor(property_c), y = assessed_value)) +
   scale_y_continuous(labels = function(x) format(x, scientific = FALSE))

ggplot(data = db2) + 
  geom_boxplot(mapping = aes(x = factor(foreclose), y = assessed_value)) +
   scale_y_continuous(labels = function(x) format(x, scientific = FALSE))


#number of foreclosures
foreclosures %>% drop_na(`2015`) %>% 
  group_by(`2015`)%>% 
  tally()
  
  foreclosures %>% drop_na(`2016`) %>% 
  group_by(`2016`)%>% 
  tally()
  
  foreclosures %>% drop_na(`2017`) %>% 
  group_by(`2017`)%>% 
  tally()
  
  foreclosures %>% drop_na(`2018`) %>% 
  group_by(`2018`)%>% 
  tally()
  
  
 ggplot(db2, aes(x=assessed_value, y=sale_price, color=factor(foreclose))) +
  geom_point() +
  geom_smooth() +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
  scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) 
 

```




```{r  cmfproperty}

devtools::install_github("cmf-uchicago/cmfproperty")

library(cmfproperty)

ratios <-
  cmfproperty::reformat_data(
    data = db1,
    sale_col = "sale_price",
    assessment_col = "assessed_value",
    sale_year_col = "year_sale",
  )
  

cmfproperty::make_report(ratios, 
                         jurisdiction_name = "Detorit, Michigan")


```

```{r Cont}

library(parsnip)

#regressions

sale_1 <- lm(sale_price ~ year_sale, data=db2)

sale_1 %>% broom::tidy()

#sale_1 %>% broom::glance()

#sale_1 %>% broom::augment()

#Issues with augment no prediction column and "new_data" is missing for the models

#very, very , very low R squared not a good model
#data skewed to the left 

ggplot(sale_1 %>% broom::augment(), aes(x=.resid)) +
  geom_density(fill='navy', alpha=.6) +
   scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
  scale_x_continuous(labels = function(x) format(x, scientific = FALSE))


#trying parsnip regression with sale/foreclosure

#not sure why the sale database and assessment have a column for property class with different value. Sticing to property_c for the sales database for now. 

lm_sale <- 
 linear_reg() %>%
  set_engine("lm") %>%
  fit(sale_price ~ factor(year_sale) + factor(property_c), data = db2)

lm_sale %>% broom::tidy()

#lm_sale %>% broom::glance()

#lm_sale %>% broom::augment() gives an error of Error in augment.model_fit(.) : 
  #argument "new_data" is missing, with no default

#(lm_sale %>% broom::augment(), aes(x=.resid)) +
  #geom_density(fill='navy', alpha=.6) +
   #scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
  #scale_x_continuous(labels = function(x) format(x, scientific = FALSE))



lm_sale <- lm(sale_price ~ year_sale + factor(property_c) + assessed_value, data = db2)

lm_sale %>% broom::tidy()

#lm_sale %>% broom::glance()

#lm_sale %>% broom::augment()

#a better model with an r squared of 19% large increase

#augment works here but it doesn't have the .resid column 

#ggplot(mod_1 %>% broom::augment(), aes(x=.resid)) +
  #geom_density(fill='navy', alpha=.6) +
   #scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
  #scale_x_continuous(labels = function(x) format(x, scientific = FALSE))


# a minimally acceptable r squared
lm_sale <- 
 linear_reg() %>%
  set_engine("lm") %>%
  fit(sale_price ~ year_sale + factor(property_c) + assessed_value + tax_value + foreclose, data = db2)


lm_sale %>% broom::tidy()

#lm_sale %>% broom::glance()

#lm_sale %>% broom::augment()

#not an improvement of the previous model 

#not sure how to wrap my head around of the dummy variables for foreclosure I would think this would be a logistic regression for log odds

#trying a logistic regression not entirely sure of the interpretation but I believe glm does not return a r squared
fore_glm <- parsnip::logistic_reg()%>%
  set_engine("glm") %>%
  fit(factor(foreclose) ~ assessed_value, family=binomial(link = "logit"), data= db2)

fore_glm %>% broom::tidy()

#fore_glm %>% broom::glance()

#cont with liner regression instead

#a minimally acceptable r squared
lm_fore <- 
 linear_reg() %>%
  set_engine("lm") %>%
  fit(assessed_value ~ foreclose + sale_price + year_sale + factor(property_c), data=db2)

lm_fore %>% broom::tidy()

#lm_fore %>% broom::glance()

#lm_fore %>% broom::augment() received an error


```


